{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467a69a0",
   "metadata": {},
   "source": [
    "<h1>Combining Multiple Datasets </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e07d0",
   "metadata": {},
   "source": [
    "1. Code-Mixed Hinglish Hate Speech Detection Dataset : https://www.kaggle.com/datasets/sharduldhekane/code-mixed-hinglish-hate-speech-detection-dataset\n",
    "2. Hinglish Hate Speech with Sentiment and Emotion : https://www.kaggle.com/datasets/shreyat22/hinglish-hate-speech-with-sentiment-and-emotion\n",
    "3. Davidson et al. (2017): https://www.kaggle.com/datasets/eldrich/hate-speech-offensive-tweets-by-davidson-et-al\n",
    "4. Thar Dataset: https://www.kaggle.com/datasets/aakash941/thar-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24042648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 columns: ['text', 'hate_label', 'source', 'profanity_score', 'language', 'dataset_version', 'combined_date', 'text_length', 'word_count']\n",
      "Dataset 2 columns: ['text', 'hate_label', 'source', 'profanity_score', 'language', 'dataset_version', 'combined_date', 'text_length', 'word_count', 'sentiment', 'emotion']\n",
      "Dataset 3 columns: ['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither', 'class', 'tweet']\n",
      "Dataset 4 columns: ['Identifier', 'Comment', 'SubTask1', 'SubTask2']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ===== Step 1: Load your datasets =====\n",
    "# Replace with your actual paths\n",
    "file1 = './Core/combined_hate_speech_dataset.csv'\n",
    "file2 = './Core/hate_speech_with_sentiment_emotion_new.csv'\n",
    "file3 = './Core/offensive-tweets-by-davidson.csv'\n",
    "file4 = './Core/THAR-Dataset.csv'\n",
    "\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "df3 = pd.read_csv(file3)\n",
    "df4 = pd.read_csv(file4)\n",
    "\n",
    "# ===== Step 2: Check columns =====\n",
    "print(\"Dataset 1 columns:\", df1.columns.tolist())\n",
    "print(\"Dataset 2 columns:\", df2.columns.tolist())\n",
    "print(\"Dataset 3 columns:\", df3.columns.tolist())\n",
    "print(\"Dataset 4 columns:\", df4.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8e781",
   "metadata": {},
   "source": [
    " 1 = hate, 0 = non-hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7288c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "df1_clean = df1[['text', 'hate_label']].copy()\n",
    "df1_clean.rename(columns={'hate_label': 'label'}, inplace=True)\n",
    "\n",
    "# Dataset 2\n",
    "df2_clean = df2[['text', 'hate_label']].copy()\n",
    "df2_clean.rename(columns={'hate_label': 'label'}, inplace=True)\n",
    "\n",
    "# Dataset 3\n",
    "df3_clean = df3[['tweet', 'class']].copy()\n",
    "df3_clean.rename(columns={'tweet': 'text', 'class': 'label'}, inplace=True)\n",
    "df3_clean = df3_clean[df3_clean['label'].isin([0, 2])]\n",
    "df3_clean['label'] = df3_clean['label'].map({0: 1, 2: 0})  # 1 = hate, 0 = non-hate\n",
    "\n",
    "# Dataset 4\n",
    "df4_clean = df4[['Comment', 'SubTask1']].copy()\n",
    "df4_clean.rename(columns={'Comment': 'text', 'SubTask1': 'label'}, inplace=True)\n",
    "df4_clean['label'] = df4_clean['label'].apply(lambda x: 1 if str(x).strip().lower() == 'antireligion' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc53f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 columns: ['text', 'label']\n",
      "Dataset 2 columns: ['text', 'label']\n",
      "Dataset 3 columns: ['text', 'label']\n",
      "Dataset 4 columns: ['text', 'label']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Dataset 1 columns:\", df1_clean.columns.tolist())\n",
    "print(\"Dataset 2 columns:\", df2_clean.columns.tolist())\n",
    "print(\"Dataset 3 columns:\", df3_clean.columns.tolist())\n",
    "print(\"Dataset 4 columns:\", df4_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddac325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 unique labels: [0 1]\n",
      "Dataset 2 unique labels: [1 0]\n",
      "Dataset 3 unique classes: [0 1]\n",
      "Dataset 4 unique labels: [1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset 1 unique labels:\", df1_clean['label'].unique())\n",
    "print(\"Dataset 2 unique labels:\", df2_clean['label'].unique())\n",
    "print(\"Dataset 3 unique classes:\", df3_clean['label'].unique())\n",
    "print(\"Dataset 4 unique labels:\", df4_clean['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d649fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    15825\n",
      "1    13725\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    15825\n",
      "1    13725\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    4163\n",
      "1    1430\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    6095\n",
      "1    5454\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df1_clean['label'].value_counts())\n",
    "print(df2_clean['label'].value_counts())\n",
    "print(df3_clean['label'].value_counts())\n",
    "print(df4_clean['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "311618eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (46681, 2)\n",
      "label\n",
      "0    26076\n",
      "1    20605\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==== Combine all datasets ====\n",
    "combined_df = pd.concat([df1_clean, df2_clean, df3_clean, df4_clean], ignore_index=True)\n",
    "\n",
    "# Clean text & remove duplicates\n",
    "combined_df.dropna(subset=['text', 'label'], inplace=True)\n",
    "combined_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "print(\"Combined dataset shape:\", combined_df.shape)\n",
    "print(combined_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c1320",
   "metadata": {},
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a125d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape: (52152, 2)\n",
      "label\n",
      "1    26076\n",
      "0    26076\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = combined_df[combined_df['label'] == 0]\n",
    "df_minority = combined_df[combined_df['label'] == 1]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "                                 replace=True,\n",
    "                                 n_samples=len(df_majority),\n",
    "                                 random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced dataset shape:\", df_balanced.shape)\n",
    "print(df_balanced['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a9e69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.to_csv('./Processed/final_hate_speech_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1e45b",
   "metadata": {},
   "source": [
    "Test-Train Seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e130d788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (41721, 2)\n",
      "Test set shape: (10431, 2)\n",
      "\n",
      "Label distribution in training set:\n",
      "label\n",
      "1    0.500012\n",
      "0    0.499988\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Label distribution in test set:\n",
      "label\n",
      "0    0.500048\n",
      "1    0.499952\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split into train (80%) and test (20%)\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced,\n",
    "    test_size=0.2,\n",
    "    stratify=df_balanced['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(train_df['label'].value_counts(normalize=True))\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(test_df['label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "698b2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./Processed/train.csv', index=False)\n",
    "test_df.to_csv('./Processed/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
